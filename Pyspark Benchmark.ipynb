{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37a5246d-aa48-47de-8f7f-3aeecf91598e",
   "metadata": {},
   "source": [
    "# Koalas(pyspark) Benchmark against Pandas\n",
    "## Data Collection\n",
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6c9c89-93d0-4f7d-a09f-225fb3a271eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyspark\n",
    "#!pip install \"pyarrow>=4.0.0\" --prefer-binary\n",
    "#!pip install koalas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec71f6cb-bdad-4c15-b72e-0046b2ae7f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "%lsmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da01b193-b08e-479f-a476-d1ad22798797",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np\n",
    "import time\n",
    "import logging\n",
    "import argparse\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import os\n",
    "import pyspark\n",
    "import timeit\n",
    "from contextlib import contextmanager\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType,LongType,FloatType,DoubleType, TimestampType\n",
    "from pyspark.sql.types import ArrayType, DoubleType, BooleanType\n",
    "from pyspark.sql.functions import col,array_contains\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae17ebd-88d0-4fa3-9d83-52d6adcfceaf",
   "metadata": {},
   "source": [
    "### Taxi Dataset\n",
    "https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d6c3ef-2ad4-43a0-9ec3-dbbaf8ae6b66",
   "metadata": {},
   "source": [
    "# define the folder you want to download the data to\n",
    "output_path = \"C:\\\\taxi\"\n",
    "\n",
    "# The most recent month for which data is available.  (Check the web site: linked above to see if new data has been made available.)\n",
    "max_month = \"2009-12\"\n",
    "\n",
    "all_months = []\n",
    "for y in range(2000, 2050):\n",
    "  for m in range(1, 13):\n",
    "    month = \"{0}-{1:02d}\".format(y, m)\n",
    "    all_months.append(month)\n",
    "\n",
    "#Define the specific taxi Dataset you want and replace the minimum date as needed\n",
    "taxi_types = {\n",
    "  'yellow': (\"2009-01\", max_month)#,\n",
    "  #'green':  (\"2013-08\", max_month),\n",
    "  #'fhv':    (\"2015-01\", max_month)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd4c7f6-209b-4c37-a3bb-70bc348bdd3a",
   "metadata": {},
   "source": [
    "for k in taxi_types.keys():\n",
    "  print(\"Processing \\\"{0}\\\"  ({1}  through  {2})\".format(k, taxi_types[k][0], taxi_types[k][1]))\n",
    "  months = [x for x in all_months if x >= taxi_types[k][0] and x <= taxi_types[k][1]]\n",
    "  \n",
    "  type_path = \"{0}/{1}\".format(output_path, k)\n",
    "  if not os.path.exists(type_path):\n",
    "    os.makedirs(type_path)\n",
    "\n",
    "    #Searches the S3 Bucked on AWS the data is stored in\n",
    "  for m in months:\n",
    "    url = \"https://s3.amazonaws.com/nyc-tlc/trip+data/{0}_tripdata_{1}.csv\".format(k, m)\n",
    "    filename = \"{0}/{1}.csv\".format(type_path, m)\n",
    "\n",
    "    # Do not download the file if we already have a copy of it\n",
    "    if not os.path.exists(filename):\n",
    "      urllib.request.urlretrieve(url, filename)\n",
    "      print(\"   Downloaded {}\".format(m))\n",
    "    else:\n",
    "      print(\"   Skipped {} (file already exists)\".format(m))\n",
    "    \n",
    "  print(\"============================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608832ad-9221-443c-81a0-5223c60bef96",
   "metadata": {},
   "source": [
    "# A Look at Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6833aae3-ac2e-4d0e-88cc-5c1bd2a4c792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WinError 123] The filename, directory name, or volume label syntax is incorrect: 'home C:\\\\taxi\\\\'\n",
      "E:\\Users\\Sven Maso\n"
     ]
    }
   ],
   "source": [
    "%cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07294ac-376c-40dc-b207-3b297c7fc00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we try to concat all the csv's we downloaded before to test read/write\n",
    "start_time = timeit.default_timer()\n",
    "all_files = glob.glob(\"E:/Users/Sven Maso/glob/*.csv\")\n",
    "df = pd.concat((pd.read_csv(f) for f in all_files))\n",
    "print(df)\n",
    "elapsed = timeit.default_timer() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6d0ddf-af71-43d7-a6f0-01f087119fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we check how long the Cell took to compute\n",
    "elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61288035-7e89-4201-a956-139480791967",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ad42ca-a673-4cd9-90cb-722d50ce6569",
   "metadata": {},
   "source": [
    "start_time2 = timeit.default_timer()\n",
    "frame.to_csv(\"example_csv.csv\")\n",
    "elapsed2 = timeit.default_timer() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f045ab3-4b3d-4c3c-a3ea-38b19c4e998c",
   "metadata": {},
   "outputs": [],
   "source": [
    "elapsed2.show.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3542e6e2-8db9-4540-82f1-08643cc34fb7",
   "metadata": {},
   "source": [
    "# A look at Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12427bb7-b018-4399-a39b-88d04f2c7c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version: 3.2.1\n",
      "PySpark Version: 3.2.1\n",
      "Access SparkUI at: http://DESKTOP-FMCAEPI.fritz.box:4040\n"
     ]
    }
   ],
   "source": [
    "# Create SparkSession\n",
    "# we build a lokal cluster, give our app a name and give it 4GB of ram to work with\n",
    "spark = SparkSession.builder.master(\"local[1]\") \\\n",
    "                    .appName('Pyspark_Benchmark') \\\n",
    "                    .config(\"spark.driver.memory\", \"4g\")\\\n",
    "                    .getOrCreate()\n",
    "# we build a lokal cluster, give our app a name and give it 4GB of ram to work with\n",
    "sc=spark.sparkContext\n",
    "#This defines our Sparkapp\n",
    "SparkUI = spark.sparkContext.uiWebUrl\n",
    "#This generates our SparkUI\n",
    "print(\"Spark Version: \" + sc.version)\n",
    "print(\"PySpark Version: \" + pyspark.__version__) \n",
    "print(\"Access SparkUI at: \" + SparkUI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "caed5ed1-abdd-4ece-84d2-558c5f813dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This defines on which schema our dataframe needs to be build\n",
    "schema = StructType([\n",
    "    StructField(\"vendor_id\", StringType(), True),\n",
    "    StructField(\"pickup_datetime\", TimestampType(), True),\n",
    "    StructField(\"dropoff_datetime\", TimestampType(), True),\n",
    "    StructField(\"passenger_count\", IntegerType(), True),\n",
    "    StructField(\"trip_distance\", DoubleType(), True),\n",
    "    StructField(\"pickup_longitude\", DoubleType(), True),\n",
    "    StructField(\"pickup_latitude\", DoubleType(), True),\n",
    "    StructField(\"rate_code_id\", IntegerType(), True),\n",
    "    StructField(\"store_and_fwd_flag\", StringType(), True),\n",
    "    StructField(\"dropoff_longitude\", DoubleType(), True),\n",
    "    StructField(\"dropoff_latitude\", DoubleType(), True),\n",
    "    StructField(\"payment_type\", StringType(), True),\n",
    "    StructField(\"fare_amount\", DoubleType(), True),\n",
    "    StructField(\"extra\", DoubleType(), True),\n",
    "    StructField(\"mta_tax\", DoubleType(), True),\n",
    "    StructField(\"tip_amount\", DoubleType(), True),\n",
    "    StructField(\"tolls_amount\", DoubleType(), True),\n",
    "    StructField(\"total_amount\", DoubleType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee1bdaa-f25b-4b21-bd33-cd7e15f9bfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time3 = timeit.default_timer()\n",
    "\n",
    "#here we build our dataframe on certain values, under options we could autocreate our schema with inferschema, but it would compute twice as long\n",
    "df = sc.read.csv(\"E:/Users/Sven Maso/glob/2009-01.csv\",header=True)\n",
    "      #.schema(yellowTripSchemaPre2015) \\\n",
    "      #.load(\"E:/Users/Sven Maso/glob/*.csv\")\n",
    "elapsed3 = timeit.default_timer() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a2510f-acf3-4d50-9d06-db88b6757610",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.sparkContext.read.csv(\"E:/Users/Sven Maso/glob/2009-01.csv\",header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8bac93-e2f7-4230-a9a7-4fb80c0d5a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "elapsed3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a412978-43eb-455b-b4ee-2ef4fbe13e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time4 = timeit.default_timer()\n",
    "#we want to store our Dataframe as a Parquete file which is compressed and faster than a csv\n",
    "df_with_schema.write.parquet(\"data.parquet\")\n",
    "elapsed4 = timeit.default_timer() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6790664-5e16-4b65-9c49-6428ea5c51d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "elapsed4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013f5b26-3b29-498a-9644-670928cce56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time5 = timeit.default_timer()\n",
    "parDF1=spark.read.parquet(\"data.parquet\")\n",
    "elapsed5 = timeit.default_timer() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12a3e9a-ae60-437a-8bbe-6bff96465a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "elapsed5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1ebf68-1f3a-4ebc-bb29-03d6750eb231",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_schema.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f906d4-ffdc-4168-ab91-1217407a93fd",
   "metadata": {},
   "source": [
    "## plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181e57b4-d03a-465c-b144-22e92320bdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    " \n",
    "# Creating dataset\n",
    "AI = ['BERT (110M parameters)', 'Transformer (213M parameters)', 'ELMo','Transformer (65M parameters)' , 'Transformer (213M parameters) w/ neural architecture search']\n",
    " \n",
    "data = [26, 192, 262, 1.438, 626.155]\n",
    " \n",
    " \n",
    "# Creating explode data\n",
    "explode = (0.15, 0.05, 0.0, 0.0, 0.02, )\n",
    " \n",
    "# Creating color parameters\n",
    "colors = ( \"orange\", \"cyan\", \"brown\",\n",
    "          \"grey\", \"indigo\")\n",
    " \n",
    "# Wedge properties\n",
    "wp = { 'linewidth' : 1, 'edgecolor' : \"black\" }\n",
    " \n",
    "# Creating autocpt arguments\n",
    "def func(pct, allvalues):\n",
    "    absolute = int(pct / 100.*np.sum(allvalues))\n",
    "    return \"{:.1f}%\\n({:d} g)\".format(pct, absolute)\n",
    " \n",
    "# Creating plot\n",
    "fig, ax = plt.subplots(figsize =(20, 14))\n",
    "wedges, texts, autotexts = ax.pie(data,\n",
    "                                  autopct = lambda pct: func(pct, data),\n",
    "                                  explode = explode,\n",
    "                                  labels = AI,\n",
    "                                  shadow = True,\n",
    "                                  colors = colors,\n",
    "                                  startangle = 90,\n",
    "                                  wedgeprops = wp,\n",
    "                                  )\n",
    " \n",
    "# Adding legend\n",
    "ax.legend(wedges, AI,\n",
    "          title =\"AI Model\",\n",
    "          loc =\"center right\",\n",
    "          bbox_to_anchor =(1, 0, 0.5, 2))\n",
    " \n",
    "plt.setp(autotexts, size = 12, weight =\"bold\")\n",
    "ax.set_title(\"The estimated Carbon footprint of training a Model in lbs\")\n",
    " \n",
    "# show plot\n",
    "plt.show()\n",
    "\n",
    "dict ={'AI Model':['Transformer (65M parameters)', 'Transformer (213M parameters)', 'ELMo', 'BERT (110M parameters)', 'Transformer (213M parameters) w/ neural architecture search', 'GPT-2'],\t\n",
    "                        'Date of original paper': ['Jun, 2017', 'Jun, 2017', 'Feb, 2018', 'Oct, 2018', 'Jan, 2019', 'Feb, 2019'],\n",
    "                        'Energy consumption (kWh)': ['27', '201', '275', '1,507', '656,347', '0'],\n",
    "                        'Carbon footprint (lbs of CO2e)': ['26', '192', '262', '1,438', '626,155', '0'],\n",
    "                        'Cloud compute cost (USD)':['\\$41-$140', '\\$289-$98', '\\$433-$1,472', '\\$3,751-$12,571', '\\$942,973-$3,201,722', '\\$12,902-$43,008']\n",
    "                    }\n",
    "aiframe = pd.DataFrame(data=dict)\n",
    "aiframe.set_index('AI Model')\n",
    "aiframe"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
